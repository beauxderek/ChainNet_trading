{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "import os\n",
    "from typing import Dict, Tuple, List\n",
    "import math\n",
    "import tensorflow as tf\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "import plotly.graph_objects as go\n",
    "from visualizer import Visualizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChainletPreprocessor:\n",
    "    def __init__(self, \n",
    "                 price_file_path: str,\n",
    "                 occ_file_path: str = None,\n",
    "                 amt_file_path: str = None,\n",
    "                 data_frequency: str = 'daily',\n",
    "                 start_year: int = 2024,\n",
    "                 end_year: int = 2024,\n",
    "                 batch_size: int = 100):\n",
    "        \"\"\"\n",
    "        Initialize chainlet preprocessor\n",
    "        \n",
    "        Args:\n",
    "            price_file_path: Path to price data\n",
    "            occ_file_path: Path to occurrence matrix file\n",
    "            amt_file_path: Path to amount matrix file (optional)\n",
    "            data_frequency: 'daily' or 'hourly'\n",
    "            start_year: Starting year for analysis (daily only)\n",
    "            end_year: Ending year for analysis (daily only)\n",
    "            batch_size: Size of sliding batch\n",
    "        \"\"\"\n",
    "        self.price_file_path = price_file_path\n",
    "        self.occ_file_path = occ_file_path\n",
    "        self.amt_file_path = amt_file_path\n",
    "        self.data_frequency = data_frequency\n",
    "        self.start_year = start_year\n",
    "        self.end_year = end_year\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        # Load price data and matrices on initialization\n",
    "        self.price_data = self.load_price_data()\n",
    "        self._daily_occ_data = None\n",
    "        self._daily_amt_data = None\n",
    "        self._hourly_matrices = None\n",
    "        \n",
    "        # Load matrices based on frequency\n",
    "        if self.data_frequency == 'daily':\n",
    "            if self.occ_file_path:\n",
    "                self._daily_occ_data = self.load_daily_matrix_file(self.occ_file_path)\n",
    "            if self.amt_file_path:\n",
    "                self._daily_amt_data = self.load_daily_matrix_file(self.amt_file_path)\n",
    "        else:  # hourly\n",
    "            if self.occ_file_path:\n",
    "                with open(self.occ_file_path, 'r') as f:\n",
    "                    self._hourly_matrices = self.parse_hourly_matrices(f.read())\n",
    "\n",
    "    def load_price_data(self) -> pd.DataFrame:\n",
    "        if self.data_frequency == 'daily':\n",
    "            # Keep existing daily code exactly as is\n",
    "            df = pd.read_csv(self.price_file_path)\n",
    "            df['date'] = pd.to_datetime(df['date'])\n",
    "            return df.set_index('date')\n",
    "        else:\n",
    "            # Modified hourly price loading\n",
    "            prices = []\n",
    "            with open(self.price_file_path, 'r') as f:\n",
    "                for line in f:\n",
    "                    if line.strip():\n",
    "                        timestamp, price = line.strip().split()\n",
    "                        # Round timestamp to hour\n",
    "                        hour_ts = int(timestamp) - (int(timestamp) % 3600)\n",
    "                        prices.append({\n",
    "                            'hour_timestamp': hour_ts,\n",
    "                            'price': float(price),\n",
    "                            'date': pd.to_datetime(hour_ts, unit='s')  # Keep date for compatibility\n",
    "                        })\n",
    "            df = pd.DataFrame(prices)\n",
    "            # Create both indices for flexible lookup\n",
    "            df = df.set_index('date')\n",
    "            df['hour_timestamp'] = df['hour_timestamp']  # Keep hour timestamp as column\n",
    "            return df\n",
    "\n",
    "    def load_daily_matrix_file(self, filepath: str) -> pd.DataFrame:\n",
    "        \"\"\"Load and parse daily chainlet data file\"\"\"\n",
    "        df = pd.read_csv(filepath, sep='\\t')\n",
    "        # Keep only year, day, totaltx and chainlet columns\n",
    "        chainlet_cols = [col for col in df.columns if ':' in str(col)]\n",
    "        required_cols = ['year', 'day', 'totaltx'] + chainlet_cols\n",
    "        return df[required_cols]\n",
    "\n",
    "    def construct_matrix_from_row(self, row) -> Tuple[np.ndarray, float]:\n",
    "        \"\"\"Convert tab-delimited chainlet row into matrix\"\"\"\n",
    "        matrix = np.zeros((20, 20))\n",
    "        total_tx = row['totaltx']\n",
    "        \n",
    "        for col in row.index:\n",
    "            if ':' in str(col):\n",
    "                i, j = map(int, col.split(':'))\n",
    "                if i < 20 and j < 20:  # Ensure within bounds\n",
    "                    matrix[i][j] = row[col]\n",
    "        \n",
    "        return matrix, total_tx\n",
    "\n",
    "    def parse_hourly_matrices(self, content: str) -> Dict[int, Tuple[np.ndarray, np.ndarray]]:\n",
    "        \"\"\"\n",
    "        Parse and aggregate hourly matrices, grouping by hour\n",
    "        Returns dict mapping hour timestamp -> (aggregated_occ_matrix, aggregated_amt_matrix)\n",
    "        \"\"\"\n",
    "        # Track matrices and transactions per hour\n",
    "        hourly_matrices = {}  # hour_timestamp -> (occ_sum, amt_sum, tx_count)\n",
    "        lines = content.strip().split('\\n')\n",
    "        i = 0\n",
    "        \n",
    "        while i < len(lines):\n",
    "            if not lines[i].strip():\n",
    "                i += 1\n",
    "                continue\n",
    "                \n",
    "            # Parse timestamp and round down to hour\n",
    "            parts = lines[i].strip().split()\n",
    "            if len(parts) == 2:\n",
    "                timestamp = int(parts[1])\n",
    "                hour_timestamp = timestamp - (timestamp % 3600)  # Round down to hour\n",
    "                i += 1\n",
    "                \n",
    "                occ_matrix = np.zeros((20, 20))\n",
    "                amt_matrix = np.zeros((20, 20))\n",
    "                \n",
    "                # Parse occurrence matrix\n",
    "                if i < len(lines) and 'tx_count_matrix' in lines[i]:\n",
    "                    entries = lines[i].split('\\t')[1:]\n",
    "                    for entry in entries:\n",
    "                        if ':' in entry:\n",
    "                            coords, count = entry.split(':')\n",
    "                            x, y = map(int, coords.split(','))\n",
    "                            occ_matrix[x][y] = int(count)\n",
    "                    i += 1\n",
    "                    \n",
    "                # Parse amount matrix\n",
    "                if i < len(lines) and 'tx_weight_matrix' in lines[i]:\n",
    "                    entries = lines[i].split('\\t')[1:]\n",
    "                    for entry in entries:\n",
    "                        if ':' in entry:\n",
    "                            coords, weight = entry.split(':')\n",
    "                            x, y = map(int, coords.split(','))\n",
    "                            amt_matrix[x][y] = float(weight)\n",
    "                    i += 1\n",
    "                    \n",
    "                # Aggregate matrices by hour\n",
    "                if hour_timestamp not in hourly_matrices:\n",
    "                    hourly_matrices[hour_timestamp] = (occ_matrix, amt_matrix, 1)\n",
    "                else:\n",
    "                    prev_occ, prev_amt, count = hourly_matrices[hour_timestamp]\n",
    "                    hourly_matrices[hour_timestamp] = (\n",
    "                        prev_occ + occ_matrix,\n",
    "                        prev_amt + amt_matrix,\n",
    "                        count + 1\n",
    "                    )\n",
    "            else:\n",
    "                i += 1\n",
    "\n",
    "        # Normalize aggregated matrices by number of transactions per hour\n",
    "        final_matrices = {}\n",
    "        for hour_ts, (occ_sum, amt_sum, count) in hourly_matrices.items():\n",
    "            final_matrices[hour_ts] = (occ_sum / count, amt_sum / count)\n",
    "            \n",
    "        return final_matrices\n",
    "\n",
    "    def get_matrices_for_period(self, period, threshold: int = 0) -> Tuple[np.ndarray, np.ndarray]:\n",
    "        \"\"\"Get matrices for a specific period\"\"\"\n",
    "        if self.data_frequency == 'daily':\n",
    "            year = period.year\n",
    "            day = period.dayofyear\n",
    "            \n",
    "            # Initialize matrices\n",
    "            occ_matrix = np.zeros((20, 20))\n",
    "            amt_matrix = np.zeros((20, 20))\n",
    "            total_tx = 1.0  # Default to avoid division by zero\n",
    "            \n",
    "            # Get occurrence matrix\n",
    "            if self._daily_occ_data is not None:\n",
    "                row = self._daily_occ_data[\n",
    "                    (self._daily_occ_data['year'] == year) & \n",
    "                    (self._daily_occ_data['day'] == day)\n",
    "                ]\n",
    "                if not row.empty:\n",
    "                    occ_matrix, total_tx = self.construct_matrix_from_row(row.iloc[0])\n",
    "            \n",
    "            # Get amount matrix if available\n",
    "            if self._daily_amt_data is not None:\n",
    "                row = self._daily_amt_data[\n",
    "                    (self._daily_amt_data['year'] == year) & \n",
    "                    (self._daily_amt_data['day'] == day)\n",
    "                ]\n",
    "                if not row.empty:\n",
    "                    amt_matrix, _ = self.construct_matrix_from_row(row.iloc[0])\n",
    "            \n",
    "            # Apply threshold filtering\n",
    "            if threshold > 0:\n",
    "                mask = occ_matrix < threshold\n",
    "                occ_matrix[mask] = 0\n",
    "                amt_matrix[mask] = 0\n",
    "            \n",
    "            # Normalize by total transactions\n",
    "            if total_tx > 0:\n",
    "                occ_matrix = occ_matrix / total_tx\n",
    "                amt_matrix = amt_matrix / total_tx\n",
    "                \n",
    "        else:  # hourly\n",
    "            timestamp = int(period)\n",
    "            if timestamp in self._hourly_matrices:\n",
    "                occ_matrix, amt_matrix = self._hourly_matrices[timestamp]\n",
    "                \n",
    "                # Apply threshold filtering\n",
    "                if threshold > 0:\n",
    "                    mask = occ_matrix < threshold\n",
    "                    occ_matrix[mask] = 0\n",
    "                    amt_matrix[mask] = 0\n",
    "                \n",
    "                # Normalize by total transactions\n",
    "                total_tx = occ_matrix.sum()\n",
    "                if total_tx > 0:\n",
    "                    occ_matrix = occ_matrix / total_tx\n",
    "                    amt_matrix = amt_matrix / total_tx\n",
    "            else:\n",
    "                return np.zeros((20, 20)), np.zeros((20, 20))\n",
    "        \n",
    "        return occ_matrix, amt_matrix\n",
    "\n",
    "    def create_feature_window(self,\n",
    "                            timestamp_or_date,\n",
    "                            window_size: int,\n",
    "                            threshold: int = 0,\n",
    "                            aggregation_allowed: bool = True,\n",
    "                            include_price: bool = True,\n",
    "                            include_occ: bool = True,\n",
    "                            include_amt: bool = True) -> np.ndarray:\n",
    "        \"\"\"Create feature window for period\"\"\"\n",
    "        if not include_occ and not include_amt:\n",
    "            raise ValueError(\"Must include at least one of occurrence or amount matrices\")\n",
    "            \n",
    "        # Get time periods for window\n",
    "        if self.data_frequency == 'daily':\n",
    "            end_date = pd.to_datetime(timestamp_or_date)\n",
    "            periods = pd.date_range(end=end_date, periods=window_size, freq='D')\n",
    "        else:\n",
    "            end_ts = int(timestamp_or_date)\n",
    "            timestamps = np.arange(\n",
    "                end_ts - (window_size-1) * 3600,\n",
    "                end_ts + 3600,\n",
    "                3600\n",
    "            )\n",
    "            periods = timestamps\n",
    "\n",
    "        # Initialize feature arrays\n",
    "        prices = []\n",
    "        occ_data = None\n",
    "        amt_data = None\n",
    "        \n",
    "        for period in periods:\n",
    "            # Get matrices for period\n",
    "            occ_matrix, amt_matrix = self.get_matrices_for_period(period, threshold)\n",
    "            \n",
    "            if include_price:\n",
    "                if self.data_frequency == 'daily':\n",
    "                    period_dt = period\n",
    "                    try:\n",
    "                        price = self.price_data.loc[period_dt]['price']\n",
    "                        prices.append(price)\n",
    "                    except KeyError:\n",
    "                        continue\n",
    "                else:\n",
    "                    hour_ts = int(period) - (int(period) % 3600)\n",
    "                    matching_prices = self.price_data[self.price_data['hour_timestamp'] == hour_ts]\n",
    "                    if not matching_prices.empty:\n",
    "                        prices.append(matching_prices.iloc[0]['price'])\n",
    "            \n",
    "            # Process occurrence matrix\n",
    "            if include_occ:\n",
    "                flat_occ = occ_matrix.flatten()\n",
    "                if aggregation_allowed:\n",
    "                    if occ_data is None:\n",
    "                        occ_data = flat_occ\n",
    "                    else:\n",
    "                        occ_data += flat_occ\n",
    "                else:\n",
    "                    if occ_data is None:\n",
    "                        occ_data = flat_occ.reshape(1, -1)\n",
    "                    else:\n",
    "                        occ_data = np.vstack([occ_data, flat_occ.reshape(1, -1)])\n",
    "            \n",
    "            # Process amount matrix\n",
    "            if include_amt:\n",
    "                flat_amt = amt_matrix.flatten()\n",
    "                if aggregation_allowed:\n",
    "                    if amt_data is None:\n",
    "                        amt_data = flat_amt\n",
    "                    else:\n",
    "                        amt_data += flat_amt\n",
    "                else:\n",
    "                    if amt_data is None:\n",
    "                        amt_data = flat_amt.reshape(1, -1)\n",
    "                    else:\n",
    "                        amt_data = np.vstack([amt_data, flat_amt.reshape(1, -1)])\n",
    "\n",
    "        # Combine features\n",
    "        features = []\n",
    "        if include_occ and occ_data is not None:\n",
    "            features.append(occ_data)\n",
    "        if include_amt and amt_data is not None:\n",
    "            features.append(amt_data)\n",
    "            \n",
    "        if not features:\n",
    "            return np.array([])\n",
    "            \n",
    "        if aggregation_allowed:\n",
    "            combined = np.concatenate(features)\n",
    "            if include_price and prices:\n",
    "                return np.append(combined, np.mean(prices))\n",
    "            return combined\n",
    "        else:\n",
    "            combined = np.column_stack(features) if len(features) > 1 else features[0]\n",
    "            if include_price and prices:\n",
    "                return np.column_stack([combined, prices])\n",
    "            return combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All features shape: (801,)\n",
      "No price features shape: (800,)\n"
     ]
    }
   ],
   "source": [
    "# Test code\n",
    "hourly_preprocessor = ChainletPreprocessor(\n",
    "    price_file_path='1h_interval_price_data.txt',\n",
    "    occ_file_path='2024_output_matrices.txt',\n",
    "    data_frequency='hourly'\n",
    ")\n",
    "\n",
    "# Test with different combinations\n",
    "# All features\n",
    "hourly_features = hourly_preprocessor.create_feature_window(\n",
    "    1704068978,  # Example timestamp\n",
    "    window_size=24,\n",
    "    threshold=0,\n",
    "    include_price=True,\n",
    "    include_occ=True,\n",
    "    include_amt=True\n",
    ")\n",
    "print(\"All features shape:\", hourly_features.shape)\n",
    "\n",
    "# Just matrices without price\n",
    "hourly_features_no_price = hourly_preprocessor.create_feature_window(\n",
    "    1704068978,\n",
    "    window_size=24,\n",
    "    threshold=0,\n",
    "    include_price=False,\n",
    "    include_occ=True,\n",
    "    include_amt=True\n",
    ")\n",
    "print(\"No price features shape:\", hourly_features_no_price.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(800,)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "daily_preprocessor = ChainletPreprocessor(\n",
    "    start_year=2012,\n",
    "    end_year=2016,\n",
    "    price_file_path='price_data.csv',\n",
    "    occ_file_path='dailyOccmatrices.txt',\n",
    "    amt_file_path='dailyAmmatrices.txt',\n",
    "    data_frequency='daily'\n",
    ")\n",
    "\n",
    "daily_features = daily_preprocessor.create_feature_window(\n",
    "    timestamp_or_date='2012-02-01',\n",
    "    window_size=24,\n",
    "    threshold=0,\n",
    "    include_price=False,\n",
    "    include_occ=True,\n",
    "    include_amt=True\n",
    ")\n",
    "daily_features.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Absolute Price Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseModel:\n",
    "    def __init__(self, params=None):\n",
    "        self.params = {\n",
    "            'learning_rate': 0.01,\n",
    "            'batch_size': 2,\n",
    "            'hidden_dim_1': 8,\n",
    "            'hidden_dim_2': 8,\n",
    "            'dropout': 0.2,\n",
    "            'num_epochs': 100,\n",
    "            'display_step': 10\n",
    "        } if params is None else params\n",
    "        \n",
    "        self.model = None\n",
    "        self.scaler = MinMaxScaler()\n",
    "        \n",
    "    def build_model(self, input_shape):\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    def train(self, train_X, train_y):\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    def predict(self, test_X):\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CoinWorksRNN(BaseModel):\n",
    "    def __init__(self, params=None):\n",
    "        super().__init__(params)\n",
    "        self.model = None\n",
    "        self.stateful_model = None  # For predictions\n",
    "        \n",
    "    def get_nn(self, window_size, batch_size):\n",
    "        \"\"\"Recreate CoinWorks' exact architecture\"\"\"\n",
    "        model = Sequential([\n",
    "            LSTM(self.params['hidden_dim_1'], \n",
    "                 batch_input_shape=(batch_size, 1, window_size),\n",
    "                 stateful=True,\n",
    "                 return_sequences=True),\n",
    "            Dropout(self.params['dropout']),\n",
    "            LSTM(self.params['hidden_dim_2'],\n",
    "                 batch_input_shape=(batch_size, 1, window_size),\n",
    "                 stateful=True),\n",
    "            Dropout(self.params['dropout']),\n",
    "            Dense(self.params['prediction_horizon'])\n",
    "        ])\n",
    "        model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "        return model\n",
    "    \n",
    "    def re_define_model(self, model, window_size, batch_size):\n",
    "        \"\"\"Create prediction model with batch_size=1\"\"\"\n",
    "        new_model = self.get_nn(window_size, batch_size)\n",
    "        old_weights = model.get_weights()\n",
    "        new_model.set_weights(old_weights)\n",
    "        new_model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "        return new_model\n",
    "    \n",
    "    def train(self, train_X, train_y):\n",
    "        \"\"\"Follow CoinWorks' training procedure\"\"\"\n",
    "        window_size = train_X.shape[2]\n",
    "        batch_size = self.params['batch_size']\n",
    "        \n",
    "        # Initialize training model\n",
    "        self.model = self.get_nn(window_size, batch_size)\n",
    "        \n",
    "        # Training loop with state resets\n",
    "        for i in range(self.params['num_epochs']):\n",
    "            self.model.fit(train_X, train_y,\n",
    "                         epochs=1,\n",
    "                         batch_size=batch_size,\n",
    "                         verbose=2)\n",
    "            self.model.reset_states()\n",
    "            \n",
    "        # Create stateful model for predictions\n",
    "        self.stateful_model = self.re_define_model(self.model, window_size, 1)\n",
    "    \n",
    "    def predict(self, test_X):\n",
    "        \"\"\"Make predictions following CoinWorks' approach\"\"\"\n",
    "        predicted_list = []\n",
    "        \n",
    "        for i in range(len(test_X)):\n",
    "            x = test_X[i:i+1]  # Keep batch dimension\n",
    "            predicted = self.stateful_model.predict(x, batch_size=1)\n",
    "            predicted_list.append(predicted)\n",
    "            self.stateful_model.reset_states()\n",
    "            \n",
    "        return np.array(predicted_list).reshape(-1, self.params['prediction_horizon'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PricePredictionPipeline:\n",
    "    def __init__(self, model_class, preprocessor, params=None):\n",
    "        self.params = {\n",
    "            'learning_rate': 0.01,\n",
    "            'batch_size': 2,\n",
    "            'hidden_dim_1': 8,\n",
    "            'hidden_dim_2': 8,\n",
    "            'dropout': 0.2,\n",
    "            'num_epochs': 100,\n",
    "            'display_step': 10,\n",
    "            'prediction_horizon': 5,  # Added this parameter\n",
    "            'window_size': 60         # Added this parameter\n",
    "        } if params is None else {**params}\n",
    "        \n",
    "        self.model = model_class(self.params)\n",
    "        self.preprocessor = preprocessor\n",
    "        self.scaler = MinMaxScaler()\n",
    "        \n",
    "    def prepare_data(self, start_date, end_date):\n",
    "        \"\"\"\n",
    "        Prepare data using CoinWorks' windowing approach\n",
    "        Returns X shaped (n_samples, batch_size, window_size) and \n",
    "        y shaped (n_samples, prediction_horizon)\n",
    "        \"\"\"\n",
    "        window_size = self.params['window_size']\n",
    "        prediction_horizon = self.params['prediction_horizon']\n",
    "        \n",
    "        # Get features for entire period\n",
    "        features_list = []\n",
    "        target_list = []\n",
    "        dates_list = []\n",
    "        \n",
    "        current_date = start_date\n",
    "        while current_date + pd.Timedelta(days=prediction_horizon) <= end_date:\n",
    "            # Get window data\n",
    "            feature_window = self.preprocessor.create_feature_window(\n",
    "                current_date,\n",
    "                window_size=window_size,\n",
    "                include_price=True\n",
    "            )\n",
    "            \n",
    "            if feature_window.size > 0:\n",
    "                # Get target prices for prediction horizon\n",
    "                target_prices = []\n",
    "                for i in range(prediction_horizon):\n",
    "                    target_date = current_date + pd.Timedelta(days=i+1)\n",
    "                    try:\n",
    "                        price = self.preprocessor.price_data.loc[target_date]['price']\n",
    "                        target_prices.append(price)\n",
    "                    except KeyError:\n",
    "                        continue\n",
    "                \n",
    "                if len(target_prices) == prediction_horizon:\n",
    "                    features_list.append(feature_window[:-1])  # Exclude current price from features\n",
    "                    target_list.append(target_prices)\n",
    "                    dates_list.append(current_date)\n",
    "            \n",
    "            current_date += pd.Timedelta(days=1)\n",
    "        \n",
    "        # Convert to arrays\n",
    "        X = np.array(features_list)\n",
    "        y = np.array(target_list)\n",
    "        dates = np.array(dates_list)\n",
    "        \n",
    "        # Reshape X for LSTM: (samples, timesteps, features)\n",
    "        X = np.reshape(X, (X.shape[0], 1, X.shape[1]))\n",
    "        \n",
    "        return X, y, dates\n",
    "        \n",
    "    def train_and_evaluate(self, start_date, end_date):\n",
    "        \"\"\"Train model and evaluate using rolling predictions\"\"\"\n",
    "        # Convert dates\n",
    "        start_date = pd.to_datetime(start_date)\n",
    "        end_date = pd.to_datetime(end_date)\n",
    "        \n",
    "        # Prepare data\n",
    "        X, y, dates = self.prepare_data(start_date, end_date)\n",
    "        \n",
    "        # Split into train/test\n",
    "        split_idx = int(len(X) * 0.8)\n",
    "        train_X, test_X = X[:split_idx], X[split_idx:]\n",
    "        train_y, test_y = y[:split_idx], y[split_idx:]\n",
    "        test_dates = dates[split_idx:]\n",
    "        \n",
    "        # Scale features\n",
    "        train_X_reshaped = train_X.reshape(-1, train_X.shape[-1])\n",
    "        test_X_reshaped = test_X.reshape(-1, test_X.shape[-1])\n",
    "        \n",
    "        X_scaler = self.scaler.fit(train_X_reshaped)\n",
    "        train_X_scaled = X_scaler.transform(train_X_reshaped).reshape(train_X.shape)\n",
    "        test_X_scaled = X_scaler.transform(test_X_reshaped).reshape(test_X.shape)\n",
    "        \n",
    "        # Train model\n",
    "        self.model.train(train_X_scaled, train_y)\n",
    "        \n",
    "        # Get predictions\n",
    "        predictions = self.model.predict(test_X_scaled)\n",
    "        \n",
    "        # Calculate metrics for each horizon\n",
    "        metrics = []\n",
    "        for i in range(self.params['prediction_horizon']):\n",
    "            horizon_rmse = np.sqrt(mean_squared_error(test_y[:, i], predictions[:, i]))\n",
    "            horizon_mae = mean_absolute_error(test_y[:, i], predictions[:, i])\n",
    "            metrics.append({\n",
    "                'horizon': i+1,\n",
    "                'rmse': horizon_rmse,\n",
    "                'mae': horizon_mae\n",
    "            })\n",
    "        \n",
    "        # Create results DataFrame\n",
    "        results_df = pd.DataFrame({\n",
    "            'date': test_dates,\n",
    "            'actual_price': test_y[:, 0],  # First horizon actual prices\n",
    "            'predicted_price': predictions[:, 0]  # First horizon predictions\n",
    "        })\n",
    "        \n",
    "        # Add predictions for other horizons\n",
    "        for i in range(1, self.params['prediction_horizon']):\n",
    "            results_df[f'actual_price_h{i+1}'] = test_y[:, i]\n",
    "            results_df[f'predicted_price_h{i+1}'] = predictions[:, i]\n",
    "        \n",
    "        return results_df, metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For daily data testing\n",
    "params = {\n",
    "   'learning_rate': 0.01,\n",
    "   'batch_size': 2,\n",
    "   'hidden_dim_1': 128,\n",
    "   'hidden_dim_2': 64,\n",
    "   'dropout': 0.2,\n",
    "   'num_epochs': 100,\n",
    "   'display_step': 10,\n",
    "   'prediction_horizon': 5,\n",
    "   'window_size': 60\n",
    "}\n",
    "\n",
    "# Initialize with daily data\n",
    "daily_preprocessor = ChainletPreprocessor(\n",
    "   price_file_path='price_data.csv',\n",
    "   occ_file_path='dailyOccmatrices.txt',\n",
    "   amt_file_path='dailyAmmatrices.txt', \n",
    "   data_frequency='daily',\n",
    "   start_year=2016,\n",
    "   end_year=2016\n",
    ")\n",
    "\n",
    "# Modify prepare_data method to handle empty feature lists\n",
    "def prepare_data(self, start_date, end_date):\n",
    "   window_size = self.params['window_size']\n",
    "   prediction_horizon = self.params['prediction_horizon']\n",
    "   \n",
    "   features_list = []\n",
    "   target_list = []\n",
    "   dates_list = []\n",
    "   \n",
    "   current_date = pd.to_datetime(start_date)\n",
    "   end_date = pd.to_datetime(end_date)\n",
    "   \n",
    "   while current_date + pd.Timedelta(days=prediction_horizon) <= end_date:\n",
    "       feature_window = self.preprocessor.create_feature_window(\n",
    "           current_date,\n",
    "           window_size=window_size,\n",
    "           include_price=True\n",
    "       )\n",
    "       \n",
    "       if feature_window.size > 0:  # Only append if we got valid features\n",
    "           features_list.append(feature_window[:-1])\n",
    "           \n",
    "           # Get target prices\n",
    "           target_prices = []\n",
    "           for i in range(prediction_horizon):\n",
    "               target_date = current_date + pd.Timedelta(days=i+1)\n",
    "               try:\n",
    "                   price = self.preprocessor.price_data.loc[target_date]['price']\n",
    "                   target_prices.append(price)\n",
    "               except KeyError:\n",
    "                   break\n",
    "                   \n",
    "           if len(target_prices) == prediction_horizon:\n",
    "               target_list.append(target_prices)\n",
    "               dates_list.append(current_date)\n",
    "               \n",
    "       current_date += pd.Timedelta(days=1)\n",
    "   \n",
    "   if not features_list:  # Check if we got any valid data\n",
    "       raise ValueError(\"No valid features found in date range\")\n",
    "       \n",
    "   X = np.array(features_list)\n",
    "   y = np.array(target_list)\n",
    "   dates = np.array(dates_list)\n",
    "   \n",
    "   # Reshape X for LSTM \n",
    "   X = X.reshape(X.shape[0], 1, -1) \n",
    "   \n",
    "   return X, y, dates\n",
    "\n",
    "pipeline = PricePredictionPipeline(\n",
    "   model_class=CoinWorksRNN,\n",
    "   preprocessor=daily_preprocessor,\n",
    "   params=params\n",
    ")\n",
    "\n",
    "results_df, metrics = pipeline.train_and_evaluate(\n",
    "   start_date='2016-01-01',\n",
    "   end_date='2016-12-31'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
